---
# Preamble

## Author
author:
  name: Мишина Анастасия Алексеевна
  orcid: 0009-0002-6246-7100
  email: 1132226532@pfur.ru
  affiliation:
    - name: Российский университет дружбы народов
      country: Российская Федерация
      postal-code: 117198
      city: Москва
      address: ул. Миклухо-Маклая, д. 6
## Title
title: "Отчет по лабораторной работе №5"
subtitle: "Дисциплина: Моделирование сетей передачи данных"
## Generic options
lang: ru-RU
number-sections: true
toc: true
toc-title: "Содержание"
toc-depth: 2
## Crossref customization
crossref:
  lof-title: "Список иллюстраций"
  lot-title: "Список таблиц"
  lol-title: "Листинги"
## Bibliography
bibliography:
  - bib/cite.bib
csl: _resources/csl/gost-r-7-0-5-2008-numeric.csl
## Formats
format:
### Pdf output format
  pdf:
    toc: true
    number-sections: true
    colorlinks: false
    toc-depth: 2
    lof: true # List of figures
    lot: true # List of tables
#### Document
    documentclass: scrreprt
    papersize: a4
    fontsize: 12pt
    linestretch: 1.5
#### Language
    babel-lang: russian
    babel-otherlangs: english
#### Biblatex
    cite-method: biblatex
    biblio-style: gost-numeric
    biblatexoptions:
      - backend=biber
      - langhook=extras
      - autolang=other*
#### Misc options
    csquotes: true
    indent: true
    header-includes: |
      \usepackage{indentfirst}
      \usepackage{float}
      \floatplacement{figure}{H}
      \usepackage[math,RM={Scale=0.94},SS={Scale=0.94},SScon={Scale=0.94},TT={Scale=MatchLowercase,FakeStretch=0.9},DefaultFeatures={Ligatures=Common}]{plex-otf}
### Docx output format
  docx:
    toc: true
    number-sections: true
    toc-depth: 2
---

# Цель работы

Основной целью работы является знакомство с принципами работы дисциплины очереди Token Bucket Filter, которая формирует входящий/исходящий трафик для ограничения пропускной способности, а также получение навыков моделирования и исследования поведения трафика посредством проведения интерактивного и воспроизводимого экспериментов в Mininet.

# Задание

1. Задайте топологию, состоящую из двух хостов и двух коммутаторов с назначенной по умолчанию mininet сетью 10.0.0.0/8.
2. Проведите интерактивные эксперименты по ограничению пропускной способности сети с помощью TBF в эмулируемой глобальной сети.
3. Самостоятельно реализуйте воспроизводимые эксперимент по применению TBF для ограничения пропускной способности. Постройте соответствующие графики.

# Теоретическое введение

Mininet[@mininet] -- это эмулятор компьютерной сети. Под компьютерной сетью подразумеваются простые компьютеры — хосты, коммутаторы, а так же OpenFlow-контроллеры. С помощью простейшего синтаксиса в примитивном интерпретаторе команд можно разворачивать сети из произвольного количества хостов, коммутаторов в различных топологиях и все это в рамках одной виртуальной машины(ВМ). На всех хостах можно изменять сетевую конфигурацию, пользоваться стандартными утилитами(ifconfig, ping) и даже получать доступ к терминалу. На коммутаторы можно добавлять различные правила и маршрутизировать трафик.

# Выполнение лабораторной работы

## Запуск лабораторной топологии

Запустим виртуальную среду с mininet. Из основной ОС подключимся к виртуальной машине. В виртуальной машине mininet при необходимости исправим права запуска X-соединения. Скопируем значение куки (MIT magic cookie) своего пользователя mininet в файл для пользователя root.

Зададим простейшую топологию, состоящую из двух хостов и коммутатора с назначенной по умолчанию mininet сетью 10.0.0.0/8. На хостах h1 и h2 и на коммутаторах s1, s2 введем команду ifconfig, чтобы отобразить информацию, относящуюся к их сетевым интерфейсам и назначенным им IP-адресам. В дальнейшем при работе с NETEM и командой tc будут использоваться интерфейсы h1-eth0, h2-eth0 и s1-eth2 ([рис. @fig-001]).

![ifconfig на хостах h1, h2 и на коммутаторах s1, s2](image/1.png){#fig-001 width=70%} 

Проверим подключение между хостами h1 и h2 с помощью команды ping с параметром -c 6 ([рис. @fig-002]).

![Проверка подключения между хостами](image/2.png){#fig-002 width=70%}

В терминале хоста h2 запустим iPerf3 в режиме сервера: `iperf3 -s`. В терминале хоста h1 запустим iPerf3 в режиме клиента: `iperf3 -c 10.0.0.2`. После завершения работы iPerf3 на хосте h1 остановите iPerf3 на хосте h2, нажав Ctrl + c ([рис. @fig-003]). Видим 14-19 Gbit/sec.

![Проверка подключения между хостами](image/3.png){#fig-003 width=70%}

## Интерактивные эксперименты

### Ограничение скорости на конечных хостах

Команду tc можно применить к сетевому интерфейсу устройства для формирования исходящего трафика.Требуется ограничить скорость отправки данных с конечного хоста с помощью фильтра Token Bucket Filter (tbf).

1. Измените пропускную способность хоста h1, установив пропускную способность на 10 Гбит/с на интерфейсе h1-eth0 и параметры TBF-фильтра: `sudo tc qdisc add dev h1-eth0 root tbf rate 10gbit burst 5000000 limit 15000000`.

Здесь:
- sudo: включить выполнение команды с более высокими привилегиями
безопасности;
- tc: вызвать управление трафиком Linux;
- qdisc: изменить дисциплину очередей сетевого планировщика;
- add (добавить): создать новое правило;
- dev h1-eth0 root: интерфейс, на котором будет применяться правило;
- tbf: использовать алгоритм Token Bucket Filter;
- rate: указать скорость передачи (10 Гбит/с);
- burst: количество байтов, которое может поместиться в корзину (5000000);
- limit: размер очереди в байтах (15000000).

2. Фильтр tbf требует установки значения всплеска при ограничении скорости. Это значение должно быть достаточно высоким, чтобы обеспечить установленную скорость. Она должна быть не ниже указанной частоты, делённой на HZ, где HZ — тактовая частота, настроенная как параметр ядра, и может быть извлечена с помощью следующей команды:

```
egrep '^CONFIG_HZ_[0-9]+' /boot/config-`uname -r`
```

Для расчёта значения всплеска (burst) необходимо скорость передачи (10 Гбит/с или 10 Gbps = 10,000,000,000 bps) разделить на полученное таким образом значение HZ (на хосте h1 HZ = 250):
Burst = 10,000,000,000 / 250 = 40,000,000 bits = 40,000,000 / 8 bytes = 5,000,000 bytes.

3. С помощью iPerf3 проверьте, что значение пропускной способности изменилось ([рис. @fig-004]):

- В терминале хоста h2 запустите iPerf3 в режиме сервера: `iperf3 -s`
- В терминале хоста h2 запустите iPerf3 в режиме клиента: `iperf3 -c 10.0.0.2`
- После завершения работы iPerf3 на хосте h1 остановите iPerf3 на хосте h2, нажав Ctrl + c.

![Ограничение скорости на конечных хостах](image/4.png){#fig-004 width=70%}

Видим, что теперь скорость передачи 9-9.5 Gbit/sec.

4. Удалите модифицированную конфигурацию на хосте h1: `sudo tc qdisc del dev h1-eth0 root` ([рис. @fig-005]):

![Удаление модифицированной конфигурации на хосте h1](image/5.png){#fig-005 width=70%}

### Ограничение скорости на коммутаторах

При ограничении скорости на интерфейсе s1-eth2 коммутатора s1 все сеансы связи между коммутатором s1 и коммутатором s2 будут фильтроваться в соответствии с применяемыми правилами.

1. Примените правило ограничения скорости tbf с параметрами rate=10gbit, burst=5,000,000, limit=15,000,000 к интерфейсу s1-eth2 коммутатора s1, который соединяет его с коммутатором s2: `sudo tc qdisc add dev s1-eth2 root tbf rate 10gbit burst 5000000 limit 15000000`.

2. Проверьте конфигурацию с помощью инструмента iperf3 для измерения пропускной способности ([рис. @fig-006]):

- В терминале хоста h2 запустите iPerf3 в режиме сервера: `iperf3 -s`
- В терминале хоста h2 запустите iPerf3 в режиме клиента: `iperf3 -c 10.0.0.2`
- После завершения работы iPerf3 на хосте h1 остановите iPerf3 на хосте h2, нажав Ctrl + c.

![Ограничение скорости на коммутаторах](image/6.png){#fig-006 width=70%}

Видим, что теперь скорость передачи 9.38-9.56 Gbit/sec.

3. Удалите модифицированную конфигурацию на коммутаторе s1: `sudo tc qdisc del dev s1-eth2 root` ([рис. @fig-007]).

![Удаление модифицированной конфигурации на коммутаторе s1](image/7.png){#fig-007 width=70%}

### Объединение NETEM и TBF

NETEM используется для изменения задержки, джиттера, повреждения пакетов и т.д. TBF может использоваться для ограничения скорости. Утилита tc позволяет комбинировать несколько модулей. При этом первая дисциплина очереди (qdisc1) присоединяется к корневой метке, последующие дисциплины очереди можно прикрепить к своим родителям, указав правильную метку.

1. Объедините NETEM и TBF, введя на интерфейсе s1-eth2 коммутатора s1 задержку, джиттер, повреждение пакетов и указав скорость: `sudo tc qdisc add dev s1-eth2 root handle 1: netem delay 10ms`.

Здесь ключевое слово handle задаёт дескриптор подключения, имеющий смысл очерёдности подключения разных дисциплин qdisc.

2. Убедитесь, что соединение от хоста h1 к хосту h2 имеет заданную задержку. Для этого запустите команду ping с параметром -c 4 с терминала хоста h1 ([рис. @fig-008]).

![Задержка, джиттер, повреждение пакетов на интерфейсе s1-eth2 коммутатора s1](image/8.png){#fig-008 width=70%}

3. Добавьте второе правило на коммутаторе s1, которое задаёт ограничение скорости с помощью tbf с параметрами rate=2gbit, burst=1,000,000, limit=2,000,000: `sudo tc qdisc add dev s1-eth2 parent 1: handle 2: tbf rate 2gbit burst 1000000 limit 2000000`.

4. Проверьте конфигурацию с помощью инструмента iperf3 для измерения пропускной способности ([рис. @fig-009]):

- В терминале хоста h2 запустите iPerf3 в режиме сервера: `iperf3 -s`
- В терминале хоста h2 запустите iPerf3 в режиме клиента: `iperf3 -c 10.0.0.2`
- После завершения работы iPerf3 на хосте h1 остановите iPerf3 на хосте h2, нажав Ctrl + c. В отчёте зафиксируйте результат работы iPerf3 на данном этапе проведения эксперимента.

![Объединение NETEM и TBF](image/9.png){#fig-009 width=70%}

Видим, что теперь скорость передачи 1.4-1.6 Gbit/sec.

5. Удалите модифицированную конфигурацию на коммутаторе s1: `sudo tc qdisc del dev s1-eth2 root`.

## Воспроизведение экспериментов

1. Для воспроизводимого эксперимента создадим свой каталог, в котором будут размещаться файлы эксперимента: `mkdir -p ~/work/lab_tbf`.
2. Скопируем содержимое из каталога предыдущих работ в новый каталог.

Изменим файл для эксперимента lab_netem_i.py ([рис. @fig-010]).

![Скрипт эксперимента](image/10.png){#fig-010 width=70%}

Создадим также скрипт ping_plot для визуализации результатов эксперимента ([рис. @fig-011]).

![Скрипт ping_plot](image/11.png){#fig-011 width=70%}

Обновим Makefile ([рис. @fig-012]).

![Makefile](image/12.png){#fig-012 width=70%}

Выполним эксперимент, построим график и посмотрим его ([рис. @fig-013]).

![График ping.png](image/13.png){#fig-013 width=70%}

# Выводы

В результате выполнения работы познакомились с принципами работы дисциплины очереди Token Bucket Filter, которая формирует входящий/исходящий трафик для ограничения пропускной способности, а также получили навыки моделирования и исследования поведения трафика посредством проведения интерактивного и воспроизводимого экспериментов в Mininet.

# Список литературы{.unnumbered}

::: {#refs}
:::